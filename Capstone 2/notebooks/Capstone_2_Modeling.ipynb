{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling \n",
    "This final phase of the NBA MVP Prediction Project will be concluded with modeling. In this notebook, we will recall the data from preprocessing that has been standardized and begin testing different parameters and models. Once the models have been selected, we will test each model for accuracy, recall, precision and F1 scores. Overall, the goal of the model is to predict the NBA MVP for any given season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Core packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#To surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Adjusting display settings\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "#Packages used for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load in the dataframes created from the previous phases, this notebook will be focusing in on the final dataframes created in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('../data/final_df.csv')\n",
    "testing_df = pd.read_csv('../data/testing_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the final_df dataframe\n",
    "final_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the season and player column back into the testing_df, it was dropped in the preprocessing phase but is needed to continue with modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df['season'] = final_df['season'].reindex(testing_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df['player'] = final_df['player'].reindex(testing_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the testing_df dataframe\n",
    "testing_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "le = LabelEncoder()\n",
    "player_ids = le.fit_transform(testing_df['player'])\n",
    "player_df = pd.concat([testing_df[['player','max_award_share']], pd.DataFrame({'player_id': player_ids})], axis=1)\n",
    "\n",
    "X = player_df.drop(['player','max_award_share'], axis=1)\n",
    "y= player_df['max_award_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X and y variables\n",
    "#X = testing_df.drop(['max_award_share'], axis=1)\n",
    "#y = testing_df['max_award_share']\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#le.fit(X['player'])\n",
    "#X['player_id'] = le.transform(X['player'])\n",
    "\n",
    "#unique_players = le.inverse_transform(X['player_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_id_map = dict(zip(X['player_id'], X['player']))\n",
    "y = y.map(lambda x: le.transform([player_id_map[x]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le = LabelEncoder()\n",
    "#testing_df['player_id'] = le.fit_transform(testing_df['player'])\n",
    "#unique_players = testing_df['player'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X and y variables\n",
    "#X = testing_df.drop(['max_award_share'], axis=1)\n",
    "#y = testing_df['max_award_share']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['player','Unnamed: 0','award_share_0','award_share_1'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the results more readable, we're going to add player names back into the dataframe, but first they must be converted to Numeric values for the models can still function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the X and y variables for any missing values, this dataset is expected to have missing values due to some statistical categories not being tracked in earlier seasons. We may want to consider applying the average method to those columns for future testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X has missing values: {X.isnull().values.any()}')\n",
    "print(f'y has missing values: {y.isnull().values.any()}')\n",
    "print(f'X has infinite values: {np.isinf(X).values.any()}')\n",
    "print(f'y has infinite values: {np.isinf(y).values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X has missing values: {X.isnull().values.any()}')\n",
    "print(f'y has missing values: {y.isnull().values.any()}')\n",
    "print(f'X has infinite values: {np.isinf(X).values.any()}')\n",
    "print(f'y has infinite values: {np.isinf(y).values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the models \n",
    "models = {'Random Forest Regressor': RandomForestRegressor(),\n",
    "          'Linear Regression': LinearRegression(),\n",
    "          'XGBoost': XGBRegressor(),\n",
    "          'LightGB': LGBMRegressor()\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing an empty dictionary to store testing results\n",
    "model_results = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting each model on the training and testing data, then the models will be evaluated and added to a dataframe to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    #Evaluating the performance metrics of each model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)  \n",
    "    \n",
    "    #Inverse transform of encoded player IDs back to player names\n",
    "    #predicted_mvp = le.inverse_transform(y_pred.astype(int))\n",
    "    #actual_mvp = y_test\n",
    "    #predicted_mvp = pd.Series(le.inverse_transform(y_pred.astype(int)), index=y_test.index)\n",
    "    #actual_mvp = pd.Series(le.inverse_transform(y_test.astype(int)), index=y_test.index)\n",
    "    predicted_mvp = le.inverse_transform(y_pred.astype(int))\n",
    "    actual_mvp = [player_df.iloc[i]['player'] for i in range(len(player_df))]\n",
    "    \n",
    "    #Creating a dataframe with the results of each model\n",
    "    df = pd.DataFrame({\n",
    "        'mean_squared_error': [mse] * len(y_test),\n",
    "        'r2_score': [r2] * len(y_test),\n",
    "        'season': X_test['season'].values,\n",
    "        'predicted_mvp': predicted_mvp,\n",
    "        'actual_mvp': actual_mvp\n",
    "    })\n",
    "\n",
    "    #Filtering the dataframe to only include rows where the predicted MVP is correct\n",
    "    df_correct = df[df['predicted_mvp'] == df['actual_mvp']]\n",
    "    df_correct['model_correct'] = 'Yes'\n",
    "    \n",
    "    #Filtering the dataframe to only include rows where the predicted MVP is incorrect\n",
    "    df_incorrect = df[df['predicted_mvp'] != df['actual_mvp']]\n",
    "    df_incorrect['model_correct'] = 'No'\n",
    "    \n",
    "    \n",
    "    #Concatening the correct and incorrect dataframes\n",
    "    df_final = pd.concat([df_correct, df_incorrect])\n",
    "\n",
    "    # Map encoded player_id values back to player names\n",
    "    df_final['predicted_mvp'] = le.inverse_transform(df_final['predicted_mvp'].astype(int))\n",
    "    df_final['actual_mvp'] = df_final['actual_mvp'].apply(lambda x: le.inverse_transform([x])[0])\n",
    "    \n",
    "    #Adding the model to the dataframe\n",
    "    df_final['model'] = name\n",
    "\n",
    "    #Plotting the results for the model\n",
    "    plt.scatter(df_final['predicted_mvp'], df_final['actual_mvp'])\n",
    "    plt.xlabel('Predicted MVP')\n",
    "    plt.ylabel('Actual MVP')\n",
    "    plt.title(name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_final['model'], df_final['mean_squared_error'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "\n",
    "sns.scatterplot(ax=axes[0,0], data=results_df, x='predicted_mvp', y='actual_mvp', hue='correct')\n",
    "axes[0,0].set_xlabel('Predicted MVP')\n",
    "axes[0,0].set_ylabel('Actual MVP')\n",
    "axes[0,0].set_title('Predicted vs Actual MVP')\n",
    "\n",
    "sns.boxplot(ax=axes[0,1], data=results_df, x='model', y='mean_squared_error')\n",
    "axes[0,1].set_xlabel('Model')\n",
    "axes[0,1].set_ylabel('Mean Squared Error')\n",
    "axes[0,1].set_title('Mean Squared Error by Model')\n",
    "\n",
    "sns.boxplot(ax=axes[1,0], data=results_df, x='model', y='r2_score')\n",
    "axes[1,0].set_xlabel('Model')\n",
    "axes[1,0].set_ylabel('R2 Score')\n",
    "axes[1,0].set_title('R2 Score by Model')\n",
    "\n",
    "sns.countplot(ax=axes[1,1], data=results_df, x='model_correct')\n",
    "axes[1,1].set_xlabel('Prediction Correct')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].set_title('Correct vs Incorrect Predictions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
